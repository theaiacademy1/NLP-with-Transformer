{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4185b630",
   "metadata": {},
   "source": [
    "**** \n",
    "**The Ai Academy**\n",
    "****"
   ]
  },
  {
   "attachments": {
    "Screenshot%202024-07-10%20111324.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "bd5edbd6",
   "metadata": {},
   "source": [
    "### Natural language processing with Transformers \n",
    "\n",
    "![Screenshot%202024-07-10%20111324.png](attachment:Screenshot%202024-07-10%20111324.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86495e76",
   "metadata": {},
   "source": [
    "#### What is Transformer.\n",
    "* The transformers have revolutionized NLP by providing a powerful and flexible model architecture that has outperformed previous models on many tasks. Introduced in the seminal paper \"Attention is All You Need\" by Vaswani et al., the Transformer model discards the recurrence found in traditional models like RNNs and instead relies entirely on a mechanism known as self-attention. The Key Characteristics of Transformer Architecture are, The Encoder-Decoder Framework which consists of two main components: the encoder and the decoder.\n",
    "  - **Encoder**: Converts an input sequence of tokens into a sequence of continuous representations.\n",
    "  - **Decoder**: Takes the output of the encoder and generates an output sequence, one token at a time.\n",
    "- **Self-Attention Mechanism**: Allows the model to weigh the importance of different tokens in the input sequence, enabling it to capture long-range dependencies more effectively than RNNs.\n",
    "- **Positional Encodings**: Since Transformers do not inherently capture the order of tokens, positional encodings are added to the input embeddings to provide information about the token positions in the sequence.\n",
    "- **Parallelization**: Unlike RNNs, Transformers allow for much greater parallelization, making training more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91686e2d",
   "metadata": {},
   "source": [
    "****\n",
    "### History and Evolution of Transformers in NLP\n",
    "\n",
    "Before Transformers, NLP models were primarily based on recurrent neural networks (RNNs) and convolutional neural networks (CNNs). While these models achieved significant milestones, they had limitations in handling long-range dependencies and parallelization.\n",
    "\n",
    "#### Evolutionary Milestones\n",
    "\n",
    "1. **RNNs and LSTMs**: Introduced the concept of sequential processing, but struggled with long-term dependencies due to gradient vanishing/exploding problems.\n",
    "2. **Attention Mechanisms**: Enhanced RNNs by allowing the model to focus on different parts of the input sequence, but were still constrained by sequential processing.\n",
    "3. **Transformers**: Introduced by Vaswani et al. in 2017, eliminated the recurrence by relying entirely on self-attention mechanisms, enabling better handling of long-range dependencies and parallelization.\n",
    "\n",
    "#### Impact on NLP\n",
    "\n",
    "Transformers have set new state-of-the-art results across a wide range of NLP tasks, including machine translation, text classification, and language modeling. Notable models based on Transformer architecture include:\n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**: Utilizes a bidirectional approach to pre-training, capturing context from both directions.\n",
    "- **GPT (Generative Pre-trained Transformer)**: Focuses on autoregressive language modeling, generating coherent text by predicting the next word in a sequence.\n",
    "- **T5 (Text-to-Text Transfer Transformer)**: Frames all NLP tasks as a text-to-text problem, simplifying the process of applying the model to different tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f5cca",
   "metadata": {},
   "source": [
    "*****\n",
    "### Key Concepts of the Transformer\n",
    "##### Self-Attention Mechanism: \n",
    "* At the heart of the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence when encoding or decoding sequences. This mechanism captures the dependencies between words regardless of their distance in the sequence, making it highly effective for understanding the context.\n",
    "##### Encoder-Decoder Architecture: \n",
    "The original Transformer model consists of an encoder and a decoder.\n",
    "* **Encoder:** The encoder reads the input sequence and transforms it into a set of continuous representations. It is composed of multiple layers, each containing two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
    "* **Decoder:** The decoder generates the output sequence. Like the encoder, it is composed of multiple layers. Each layer has three sub-layers: a multi-head self-attention mechanism, an encoder-decoder attention mechanism, and a position-wise fully connected feed-forward network. The encoder-decoder attention mechanism allows the decoder to focus on relevant parts of the input sequence during generation.\n",
    "##### Positional Encoding: \n",
    "* Since Transformers do not inherently understand the order of the sequence, positional encodings are added to the input embeddings to provide information about the relative positions of the tokens in the sequence.\n",
    "##### Multi-Head Attention: \n",
    "* This mechanism allows the model to focus on different parts of the input sequence simultaneously, providing multiple attention scores for each token. It improves the ability of the model to capture various aspects of the relationships between words.\n",
    "#### Detailed Components\n",
    "* **Attention Mechanisms:** Attention mechanisms are crucial for the functionality of Transformers. They allow the model to dynamically focus on different parts of the input sequence. The self-attention mechanism computes a representation of each token by considering the entire sequence using three matrices: Query (Q), Key (K), and Value (V).\n",
    "* **Feed-Forward Networks:** These are applied independently to each position in the sequence. They consist of two linear transformations with a ReLU activation in between, allowing the model to learn complex transformations of the input features.\n",
    "* **Layer Normalization:** Applied to each sub-layer, it stabilizes and accelerates the training process by normalizing the input across the features for each layer.\n",
    "#### Advantages of Transformers\n",
    "* **Parallelization:** Unlike RNNs, which process tokens sequentially, Transformers can process tokens in parallel, significantly speeding up training and inference.\n",
    "* **Long-Range Dependencies:** Transformers can effectively capture long-range dependencies in sequences, which is a limitation in traditional RNNs and LSTMs.\n",
    "* **Scalability:** Transformers scale well with large datasets and models, enabling them to learn from vast amounts of data and improve their performance on various tasks.\n",
    "#### Applications of Transformers\n",
    "* **Language Translation:** The initial application of Transformers, where they significantly improved the quality and efficiency of machine translation systems.\n",
    "* **Text Summarization:** Generating concise summaries of longer texts.\n",
    "* **Question Answering:** Providing precise answers to questions based on input text.\n",
    "* **Text Generation:** Creating coherent and contextually relevant text based on given prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a3594e",
   "metadata": {},
   "source": [
    "### Overview of Hugging Face Transformers Library and Ecosystem\n",
    "\n",
    "Hugging Face has developed a comprehensive ecosystem that simplifies the use of Transformer models in Natural Language Processing (NLP). This ecosystem includes several powerful tools and libraries designed to make it easier for researchers and developers to access, fine-tune, and deploy state-of-the-art models for a wide range of NLP tasks.\n",
    "\n",
    "#### Hugging Face Transformers Library\n",
    "\n",
    "The Hugging Face Transformers library is the centerpiece of the ecosystem. It provides an easy-to-use interface for accessing a variety of pre-trained Transformer models and fine-tuning them on custom datasets. Here are some key features of the library:\n",
    "\n",
    "- **Model Hub**: A vast repository of pre-trained models covering a wide range of NLP tasks. Users can easily search, download, and integrate these models into their applications. The Model Hub supports multiple model architectures, such as BERT, GPT-2, RoBERTa, T5, and many others.\n",
    "  \n",
    "- **Tokenizers**: The library offers efficient and versatile tokenizers that support various tokenization strategies. Tokenization is the process of converting text into numerical tokens that the models can process. Hugging Face provides tokenizers for word, subword, and character tokenization, which are optimized for speed and compatibility with different model architectures.\n",
    "\n",
    "- **Trainer API**: A high-level interface that simplifies the process of training and evaluating models. The Trainer API abstracts away much of the boilerplate code required for training models, allowing users to focus on their specific tasks. It supports features such as distributed training, mixed precision, and hyperparameter tuning.\n",
    "\n",
    "#### Hugging Face Model Hub\n",
    "\n",
    "The Model Hub is an integral part of the Hugging Face ecosystem. It hosts thousands of pre-trained models that can be used for various NLP tasks such as text classification, named entity recognition, question answering, text generation, and more. The Model Hub offers the following features:\n",
    "\n",
    "- **Search and Filter**: Users can search for models based on specific criteria such as task, architecture, dataset, language, and more. This makes it easy to find the most suitable model for a particular use case.\n",
    "  \n",
    "- **Community Contributions**: The Model Hub encourages contributions from the community. Researchers and developers can upload their pre-trained models, making them available to others. This collaborative approach fosters innovation and accelerates the adoption of new techniques.\n",
    "\n",
    "- **Documentation and Examples**: Each model on the Model Hub comes with detailed documentation and usage examples, which help users understand how to effectively utilize the models in their projects.\n",
    "\n",
    "#### Additional Libraries\n",
    "\n",
    "In addition to the Transformers library, Hugging Face provides several other libraries that enhance the overall ecosystem:\n",
    "\n",
    "- **Datasets**: This library offers a seamless way to access and preprocess datasets. It integrates with popular frameworks like Pandas and NumPy, making it easy to manipulate and transform data. The Datasets library supports a wide range of datasets and includes tools for data loading, preprocessing, and augmentation.\n",
    "\n",
    "- **Accelerate**: The Accelerate library provides tools for optimizing the training process. It includes features such as distributed training, mixed precision training, and gradient accumulation, which help to speed up training and reduce resource usage. Accelerate is designed to work seamlessly with the Transformers library, allowing users to scale their training workflows efficiently.\n",
    "\n",
    "### Detailed Components\n",
    "\n",
    "#### Model Hub\n",
    "\n",
    "The Hugging Face Model Hub is a centralized repository that hosts a wide array of pre-trained models. It provides a user-friendly interface for browsing and accessing models. Here are some specific features and benefits:\n",
    "\n",
    "- **Diverse Model Selection**: The Model Hub includes models trained on various tasks such as language translation, summarization, text generation, and more. Users can find models tailored to specific languages, domains, or datasets.\n",
    "  \n",
    "- **Versioning and Updates**: Models on the Hub are versioned, ensuring that users can access previous versions if needed. Authors of models can update their models and provide release notes, making it clear what changes or improvements have been made.\n",
    "\n",
    "- **Integration with Transformers Library**: Models from the Hub can be easily loaded using the Transformers library. This integration simplifies the process of deploying pre-trained models in applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f366b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9971315860748291}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a specific pre-trained model from the Model Hub for sentiment analysis\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "classifier = pipeline('sentiment-analysis', model=model_name)\n",
    "\n",
    "# Use the model to analyze sentiment\n",
    "result = classifier(\"I love using Hugging Face transformers!\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44347adc",
   "metadata": {},
   "source": [
    "#### Tokenizers\n",
    "\n",
    "Tokenization is a crucial step in preparing text data for input into Transformer models. The Hugging Face tokenizers library provides efficient and flexible tokenization methods. Key features include:\n",
    "\n",
    "- **Fast and Efficient**: Tokenizers are written in Rust and provide bindings for Python, ensuring they are both fast and efficient. This is particularly important when dealing with large datasets.\n",
    "  \n",
    "- **Support for Multiple Tokenization Strategies**: The library supports various tokenization methods, including byte-pair encoding (BPE), WordPiece, and sentencepiece. This flexibility allows users to choose the best method for their specific use case.\n",
    "\n",
    "- **Custom Tokenization**: Users can create custom tokenizers tailored to their specific needs, such as adding special tokens or modifying tokenization rules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88bb01cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 7592, 1010, 2129, 2024, 2017, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89897add",
   "metadata": {},
   "source": [
    "#### Trainer API\n",
    "\n",
    "The Trainer API in the Transformers library provides a high-level abstraction for training and evaluating models. It simplifies the training process by handling common tasks such as data loading, optimization, and evaluation. Key features include:\n",
    "\n",
    "- **Ease of Use**: The Trainer API reduces the amount of code needed to set up and train models, making it accessible to users with varying levels of expertise.\n",
    "  \n",
    "- **Advanced Features**: The API supports advanced training features such as distributed training across multiple GPUs, mixed precision training for faster computation, and hyperparameter tuning.\n",
    "\n",
    "- **Flexible Configuration**: Users can customize various aspects of the training process, including learning rates, batch sizes, and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488c6401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "loading configuration file config.json from cache at C:\\Users\\vedpp/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\vedpp/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\86b5e0934494bd15c9632b12f734a8a67f723594\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 1\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n",
      "  Number of trainable parameters = 109483778\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.202830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.304595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.392756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.3574597438176473, metrics={'train_runtime': 8.0547, 'train_samples_per_second': 0.372, 'train_steps_per_second': 0.372, 'total_flos': 12333330720.0, 'train_loss': 0.3574597438176473, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Custom dataset to ensure __len__ is implemented\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Assume these datasets are lists of dictionaries\n",
    "train_data = [\n",
    "    {\"input_ids\": torch.tensor([101, 7592, 1010, 2129, 2024, 2017, 1029, 102]), \"attention_mask\": torch.tensor([1, 1, 1, 1, 1, 1, 1, 1]), \"labels\": torch.tensor(1)}\n",
    "] # Your training data here\n",
    "eval_data = [\n",
    "    {\"input_ids\": torch.tensor([101, 7592, 1010, 2129, 2024, 2017, 1029, 102]), \"attention_mask\": torch.tensor([1, 1, 1, 1, 1, 1, 1, 1]), \"labels\": torch.tensor(0)}\n",
    "]   # Your evaluation data here\n",
    "\n",
    "# Wrap datasets\n",
    "train_dataset = MyDataset(train_data)\n",
    "eval_dataset = MyDataset(eval_data)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    report_to=\"none\"  # Disable the reporting warning\n",
    ")\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create a Trainer instance with custom optimizer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    optimizers=(AdamW(model.parameters(), lr=5e-5), None)  # Using PyTorch's AdamW optimizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb461d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
